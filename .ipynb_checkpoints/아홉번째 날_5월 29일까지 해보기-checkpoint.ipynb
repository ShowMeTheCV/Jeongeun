{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이번주 목표(openCV 머신러닝 공식 가이드)               \n",
    "--- \n",
    "\n",
    "- 죄송합니다.. 얼룩말 책 개정판을 못빌렸네요 ㅠㅠㅠ....         \n",
    "- 얼룩말 책과 노란책의 목차와 서술을 참고하되, 내부 내용은 [python opencv machine learning 튜토리얼](https://docs.opencv.org/master/d6/de2/tutorial_py_table_of_contents_ml.html)을 참고하였습니다. \n",
    "- 이번 기회에 공식 문서를 보고 공부하는 방법을 연습해봐도 좋겠네요!\n",
    "\n",
    "### 목차\n",
    "\n",
    "0. 머신러닝 개요\n",
    "1. K-최근접 이웃 알고리즘               \n",
    "    - K-최근접 이웃 알고리즘       \n",
    "    - 적용해보기\n",
    "3. 서포트 벡터 머신      \n",
    "    - 서포트 벡터 머신 알고리즘     \n",
    "    - 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 머신러닝 개요          \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 머신러닝 : 주어진 데이터를 분석하여 규칙성, 패턴 등을 찾고, 이를 이용하여 의미있는 정보를 추출하는 과정을 말함.\n",
    "\n",
    "- 학습,훈련(train) : 데이터로부터 규칙성을 찾아내는 과정\n",
    "- 모델 : 학습, 훈련에 의해 결정된 규칙\n",
    "- 예측, 추론 : 새로운 데이터를 학습된 모델에 입력으로 전달하고 결과를 판단하는 과정          \n",
    "\n",
    "\n",
    "- 머신러닝은 크게 __지도학습과 비지도 학습으로 구분한다.__  \n",
    "- 지도학습 : 정답을 알고 있는 데이터를 이용하여 학습을 진행하는 방식    \n",
    "- 비지도학습\n",
    "\n",
    "- 아래의 사진은 지도학습 과정을 이미지로 나타낸 것이다. \n",
    "\n",
    "![머신러닝과정](./PostingPic/9_머신러닝과정.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영상 데이터는 픽셀로 이루어져 있지만, 이 픽셀 값을 그대로 머신러닝으로 사용하지는 않는다.\n",
    "\n",
    "#### Why?\n",
    "    - 영상 데이터는 조명 변화, 객체의 이동 및 회전 등에 매우 민감하게 반응하므로\n",
    "    \n",
    "#### So,\n",
    "    - 영상의 다양한 변환에도 크게 변경되지 않는 특징 정보를 추출하여 머신 러닝 입력으로 전달함\n",
    "    - ex) 사과와 바나나 사진을 구분한다고 할 때,\n",
    "        1. 영상의 주된 색상(hue채널)\n",
    "        2. 객체 외곽선(동그란 사과와 긴 타원형인 바나나)    \n",
    "        3. 면적 비율\n",
    "        \n",
    "    - 다수의 훈련 영상에서 특징 벡터를 추출하고, 이를 이용하여 머신 러닝 알고리즘을 학습시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지도학습은 주로 __회귀 또는 분류에 사용됨__\n",
    "- 회귀 : 연속된 수치 값을 예측하는 작업. ex)학생의 키와 몸무게를 학습한 뒤, 몸무게만 주었을 때 키 예측   \n",
    "- 분류 : 이산적인 값을 결과로 출력하는 머신러닝. ex) 사과와 바나나의 구분(사과-0클래스, 바나나-1클래스)   \n",
    "\n",
    "\n",
    "- 비지도학습은 훈련 데이터의 정답에 대한 정보 없이 오로지 데이터 자체만을 이용한 학습 방식    \n",
    "- 비지도 학습은 주로 __군집화에 사용__ 됨    \n",
    "\n",
    "\n",
    "#### 학습 후 테스트를 위해 영상의 모든 프레임을 학습에 사용하지는 않고, 일부는 분리하여 테스트로 사용하기도 한다.\n",
    "\n",
    "\n",
    "#### [파이썬을 활용한 openCV 머신러닝 튜토리얼](https://docs.opencv.org/master/d6/de2/tutorial_py_table_of_contents_ml.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-최근접 이웃 알고리즘                  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. KNN 알고리즘의 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-최근접 이웃 알고리즘 : 분류 또는 회귀에 사용되는 지도학습 알고리즘       \n",
    "- K-최근접이 분류 태스크에 적용되면 : __특징 공간에서 테스트 데이터와 가장 가까운 k개의 훈련데이터를 찾고, K개의 훈련 데이터를 다수결로 판정하여 테스트 데이터의 클래스를 지정함__       \n",
    "- K-최근접이 회귀 태스크에 적용되면 : __테스트 데이터에 인접현 k개의 훈련 데이터의 평균을 테스트 데이터 값으로 설정__     \n",
    "\n",
    "![k-근접](./PostingPic/9_k최근접.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN is one of the simplest classification algorithms available for supervised learning. The idea is to search for the closest match(es) of the test data in the feature space. We will look into it with the below image.\n",
    "\n",
    "In the image, there are two families: Blue Squares and Red Triangles. We refer to each family as a Class. Their houses are shown in their town map which we call the Feature Space. You can consider a feature space as a space where all data are projected. For example, consider a 2D coordinate space. Each datum has two features, a x coordinate and a y coordinate. You can represent this datum in your 2D coordinate space, right? Now imagine that there are three features, you will need 3D space. Now consider N features: you need N-dimensional space, right? This N-dimensional space is its feature space. In our image, you can consider it as a 2D case with two features.\n",
    "\n",
    "Now consider what happens if a new member comes into the town and creates a new home, which is shown as the green circle. He should be added to one of these Blue or Red families (or classes). We call that process, Classification. How exactly should this new member be classified? Since we are dealing with kNN, let us apply the algorithm.\n",
    "\n",
    "One simple method is to check who is his nearest neighbour. From the image, it is clear that it is a member of the Red Triangle family. So he is classified as a Red Triangle. This method is called simply Nearest Neighbour classification, because classification depends only on the nearest neighbour.\n",
    "\n",
    "But there is a problem with this approach! Red Triangle may be the nearest neighbour, but what if there are also a lot of Blue Squares nearby? Then Blue Squares have more strength in that locality than Red Triangles, so just checking the nearest one is not sufficient. Instead we may want to check some k nearest families. Then whichever family is the majority amongst them, the new guy should belong to that family. In our image, let's take k=3, i.e. consider the 3 nearest neighbours. The new member has two Red neighbours and one Blue neighbour (there are two Blues equidistant, but since k=3, we can take only one of them), so again he should be added to Red family. But what if we take k=7? Then he has 5 Blue neighbours and 2 Red neighbours and should be added to the Blue family. The result will vary with the selected value of k. Note that if k is not an odd number, we can get a tie, as would happen in the above case with k=4. We would see that our new member has 2 Red and 2 Blue neighbours as his four nearest neighbours and we would need to choose a method for breaking the tie to perform classification. So to reiterate, this method is called k-Nearest Neighbour since classification depends on the k nearest neighbours.\n",
    "\n",
    "Again, in kNN, it is true we are considering k neighbours, but we are giving equal importance to all, right? Is this justified? For example, take the tied case of k=4. As we can see, the 2 Red neighbours are actually closer to the new member than the other 2 Blue neighbours, so he is more eligible to be added to the Red family. How do we mathematically explain that? We give some weights to each neighbour depending on their distance to the new-comer: those who are nearer to him get higher weights, while those that are farther away get lower weights. Then we add the total weights of each family separately and classify the new-comer as part of whichever family received higher total weights. This is called modified kNN or weighted kNN.\n",
    "\n",
    "- NN\n",
    "- KNN\n",
    "- modified k-nn(weighted k-nn)\n",
    "\n",
    "#### 고려해야 할 점\n",
    "1. new-comer로 부터 모든 이웃들의 거리를 계산해야 하기 때문에, 모든 '하우스'에 대한 정보를 알아야 함. 하우스가 많다면 그만큼 메모리, 시간 소비량이 늘어날것임         \n",
    "2. 준비 및 훈련에 거의 'zero' 타임이 걸림. 우리의 \"학습\"은 오직 데이터를 기억하는것에만 국한되기 때문임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q1. 아래의 코드를 실행시켜보고 위의 알고리즘을 바탕으로 의미를 분석해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가벼운 테스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#25개의 랜덤 하우스를 만들어냄\n",
    "#(random.randint(low, high=None, size=None, dtype=int))\n",
    "trainData = np.random.randint(0,100,(25,2)).astype(np.float32)\n",
    "\n",
    "#라벨링\n",
    "labels = np.random.randint(0,2,(25,1)).astype(np.float32)\n",
    "print(labels.ravel())\n",
    "\n",
    "red = trainData[labels.ravel()==0]\n",
    "plt.scatter(red[:,0], red[:,1], 80, 'r', '^')\n",
    "\n",
    "blue = trainData[labels.ravel()==1]\n",
    "\n",
    "#filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X')\n",
    "plt.scatter(blue[:,0], blue[:,1], 80, 'b', 's')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
      " 1.]\n",
      "result:  [[1.]]\n",
      "\n",
      "neighbours:  [[0. 1. 1.]]\n",
      "\n",
      "distance:  [[194. 205. 452.]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#25개의 랜덤 하우스를 만들어냄\n",
    "#(random.randint(low, high=None, size=None, dtype=int))\n",
    "trainData = np.random.randint(0,100,(25,2)).astype(np.float32)\n",
    "\n",
    "#라벨링\n",
    "labels = np.random.randint(0,2,(25,1)).astype(np.float32)\n",
    "print(labels.ravel())\n",
    "\n",
    "red = trainData[labels.ravel()==0]\n",
    "plt.scatter(red[:,0], red[:,1], 80, 'r', '^')\n",
    "\n",
    "blue = trainData[labels.ravel()==1]\n",
    "\n",
    "#filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X')\n",
    "plt.scatter(blue[:,0], blue[:,1], 80, 'b', 's')\n",
    "\n",
    "newcomer = np.random.randint(0,100,(1,2)).astype(np.float32)\n",
    "plt.scatter(newcomer[:,0],newcomer[:,1],80,'g','o')\n",
    "\n",
    "knn = cv.ml.KNearest_create()\n",
    "knn.train(trainData, cv.ml.ROW_SAMPLE, labels)\n",
    "\n",
    "ret, results, neighbours ,dist = knn.findNearest(newcomer, 3)\n",
    "print( \"result:  {}\\n\".format(results) )\n",
    "print( \"neighbours:  {}\\n\".format(neighbours) )\n",
    "print( \"distance:  {}\\n\".format(dist) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. 핸드라이팅 Digits을 분류하는 KNN알고리즘을 실행시켜 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OCR of Hand-written Digits\n",
    "\n",
    "Our goal is to build an application which can read handwritten digits. For this we need some training data and some test data. OpenCV comes with an image digits.png (in the folder opencv/samples/data/) which has 5000 handwritten digits (500 for each digit). Each digit is a 20x20 image. \n",
    "\n",
    "- 20*20 사이즈의 5000개 데이터(10개 종류 * 각 500개) 를 제공하고 있음\n",
    "\n",
    "So our first step is to split this image into 5000 different digit images. Then for each digit (20x20 image), we flatten it into a single row with 400 pixels. That is our feature set, i.e. intensity values of all pixels. It is the simplest feature set we can create. We use the first 250 samples of each digit as training data, and the other 250 samples as test data. So let's prepare them first.\n",
    "\n",
    "- 각 사진을 flatten하여 400개의 픽셀로 뽑아내고, 이것이 피쳐 셋이 될 것임           \n",
    "- 각 글자 500개중 250개는 훈련 데이터, 250개는 테스트 데이터로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q2. 아래의 소스코드를 실행시키거나, 직접 주석을 달아봅시다. (샘플 데이터는 opencv/samples/data 아래에 제공되고 있다고 명시하고 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.2) /tmp/pip-req-build-yw7uvgqm/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e4aed782bbfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'digits.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Now we split the image to 5000 cells, each 20x20 size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.2) /tmp/pip-req-build-yw7uvgqm/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('digits.png')\n",
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Now we split the image to 5000 cells, each 20x20 size\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]\n",
    "\n",
    "# Make it into a Numpy array: its size will be (50,100,20,20)\n",
    "x = np.array(cells)\n",
    "\n",
    "# Now we prepare the training data and test data\n",
    "train = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n",
    "test = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n",
    "\n",
    "# Create labels for train and test data\n",
    "k = np.arange(10)\n",
    "train_labels = np.repeat(k,250)[:,np.newaxis]\n",
    "test_labels = train_labels.copy()\n",
    "\n",
    "# Initiate kNN, train it on the training data, then test it with the test data with k=1\n",
    "knn = cv.ml.KNearest_create()\n",
    "knn.train(train, cv.ml.ROW_SAMPLE, train_labels)\n",
    "ret,result,neighbours,dist = knn.findNearest(test,k=5)\n",
    "\n",
    "# Now we check the accuracy of classification\n",
    "# For that, compare the result with test_labels and check which are wrong\n",
    "matches = result==test_labels\n",
    "correct = np.count_nonzero(matches)\n",
    "\n",
    "accuracy = correct*100.0/result.size\n",
    "print( accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /home/ssac23/anaconda3\n",
      "CV                       /home/ssac23/anaconda3/envs/CV\n",
      "Jeongeun                 /home/ssac23/anaconda3/envs/Jeongeun\n",
      "aiffel                *  /home/ssac23/anaconda3/envs/aiffel\n",
      "visionEnv                /home/ssac23/anaconda3/envs/visionEnv\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda info --envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our basic OCR app is ready. This particular example gave me an accuracy of 91%. One option to improve accuracy is to add more data for training, especially for the digits where we had more errors.\n",
    "\n",
    "Instead of finding this training data every time I start the application, I better save it, so that the next time, I can directly read this data from a file and start classification. This can be done with the help of some Numpy functions like np.savetxt, np.savez, np.load, etc. Please check the NumPy docs for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "np.savez('knn_data.npz',train=train, train_labels=train_labels)\n",
    "# Now load the data\n",
    "with np.load('knn_data.npz') as data:\n",
    "    print( data.files )\n",
    "    train = data['train']\n",
    "    train_labels = data['train_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my system, it takes around 4.4 MB of memory. Since we are using intensity values (uint8 data) as features, it would be better to convert the data to np.uint8 first and then save it. It takes only 1.1 MB in this case. Then while loading, you can convert back into float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR을 영어 알파벳에 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will do the same for the English alphabet, but there is a slight change in data and feature set. Here, instead of images, OpenCV comes with a data file, letter-recognition.data in opencv/samples/cpp/ folder. If you open it, you will see 20000 lines which may, on first sight, look like garbage. Actually, in each row, the first column is a letter which is our label. The next 16 numbers following it are the different features. These features are obtained from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php). You can find the details of these features in [this page](http://archive.ics.uci.edu/ml/datasets/Letter+Recognition).\n",
    "\n",
    "There are 20000 samples available, so we take the first 10000 as training samples and the remaining 10000 as test samples. We should change the letters to ascii characters because we can't work with letters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "# Load the data and convert the letters to numbers\n",
    "data= np.loadtxt('letter-recognition.data', dtype= 'float32', delimiter = ',',\n",
    "                    converters= {0: lambda ch: ord(ch)-ord('A')})\n",
    "# Split the dataset in two, with 10000 samples each for training and test sets\n",
    "train, test = np.vsplit(data,2)\n",
    "# Split trainData and testData into features and responses\n",
    "responses, trainData = np.hsplit(train,[1])\n",
    "labels, testData = np.hsplit(test,[1])\n",
    "# Initiate the kNN, classify, measure accuracy\n",
    "knn = cv.ml.KNearest_create()\n",
    "knn.train(trainData, cv.ml.ROW_SAMPLE, responses)\n",
    "ret, result, neighbours, dist = knn.findNearest(testData, k=5)\n",
    "correct = np.count_nonzero(result == labels)\n",
    "accuracy = correct*100.0/10000\n",
    "print( accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습이 된 경우 정확도 93.22% 정도가 나올 것이라고 합니다. 정확도를 올리기 위해, 더 많은 데이터를 수집하여 적용해봅시다! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q3. Exercise : 여기서 우리는 k=5 로 설정했습니다. 만약 다른 K값을 사용하면 어떤 결과가 나올까요? 정확도를 극대화할 수 있는 k를 찾을 수 있을까요? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM(서포트 벡터 머신)          \n",
    "---\n",
    "\n",
    "### A. 서포트 벡터 머신 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 서포트 벡터 머신은 __두 개의 클래스로 구성된 데이터를 가장 여유있게 분리하는 초평면을 찾는 머신러닝 알고리즘이다.__   \n",
    "- ex) 2차원 공간상의 경우는 두 클래스의 데이터를 분리하는 직선 형태로, 3차원 공간의 경우는 3차원 공간에서의 평면의 방정식이 된다.\n",
    "\n",
    "- __SVM 알고리즘은 지도학습의 일종이며, 분류와 회귀에 사용된다.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm](./PostingPic/9_svm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a)그림에서, \n",
    "    - 직선 (1)과 (2)는 삼각형 사각형을 서로 잘 분리한다.      \n",
    "    - 하지만, 1번 직선은 왼쪽으로 조금만 움직여도 아슬아슬하고, 2번 직선은 오른쪽으로 조금만 움직여도 아슬아슬하다.\n",
    "    - 이를 잘 해결하기 위해 __그림 (b)처럼 적당한 간격을 가진 초평면 (3)번 직선을 찾으려는 것이 svm이다.__  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3번 직선에서 가장 가까운 빨간색/파란색 점과의 거리를 마진(margin)이라고 한다.\n",
    "- __SVM은 이 마진을 최대화하는 초평면을 구하는 알고리즘이다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 서포트 벡터\n",
    "- 서포트 플레인\n",
    "\n",
    "Weight vector decides the orientation of decision boundary while bias point decides its location.\n",
    "\n",
    "Non-Linearly Separable Data\n",
    "Consider some data which can't be divided into two with a straight line. For example, consider an one-dimensional data where 'X' is at -3 & +3 and 'O' is at -1 & +1. Clearly it is not linearly separable. But there are methods to solve these kinds of problems. If we can map this data set with a function, f(x)=x2, we get 'X' at 9 and 'O' at 1 which are linear separable.\n",
    "\n",
    "Otherwise we can convert this one-dimensional to two-dimensional data. We can use f(x)=(x,x2) function to map this data. Then 'X' becomes (-3,9) and (3,9) while 'O' becomes (-1,1) and (1,1). This is also linear separable. In short, chance is more for a non-linear separable data in lower-dimensional space to become linear separable in higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SVM 알고리즘은 기본적으로 선형으로 분리 가능한 데이터에 적용할 수 있다.    \n",
    "2. 하지만, 선형으로 분리되지 않는 경우(아래 그림의 a에 대항)\n",
    "    - SVM 알고리즘을 적용하기 위해 __커널 트릭__ 이라는 기법을 사용한다.\n",
    "    \n",
    "![커널트릭](./PostingPic/9_커널트릭.jpg)\n",
    "    \n",
    "#### 커널트릭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a) 그림과 같이 2차원 평면상에서 분리가 어려운 경우, \n",
    "- (b) 그림처럼 데이터의 특징을 반영한 공간의 __차원을 증가시켜__ 데이터를 선형분리하는 것을 커널트릭이라 한다.\n",
    "\n",
    "- 아래의 표는 svm알고리즘에서 사용하는 커널 함수의 종류이다. \n",
    "\n",
    "![커널함수종류](./PostingPic/9_커널종류.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. 위와 동일하게, 핸드라이팅 Digits을 분류하는 SVM을 적용해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[이 튜토리얼을 참고하였습니다.](https://docs.opencv.org/master/dd/d3b/tutorial_py_svm_opencv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR of Hand-written Digits\n",
    "\n",
    "In kNN, we directly used pixel intensity as the feature vector. This time we will use Histogram of Oriented Gradients (HOG) as feature vectors.\n",
    "- 이번에는 특징 벡터를 추출하기 위해 HOG를 사용합니다.\n",
    "\n",
    "Here, before finding the HOG, we deskew the image using its second order moments. So we first define a function deskew() which takes a digit image and deskew it. Below is the deskew() function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q4.아래의 코드를 한 번 분석해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deskew는 기울기 보정이라는 의미라고 합니당\n",
    "#왜 하는지 기술적 이유를 아시는분?\n",
    "def deskew(img):\n",
    "    #배열, 이진화 이미지\n",
    "    m = cv.moments(img)\n",
    "    \n",
    "    print(m)\n",
    "    \n",
    "    if abs(m['mu02']) < 1e-2:\n",
    "        return img.copy()\n",
    "    \n",
    "    skew = m['mu11']/m['mu02']\n",
    "    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n",
    "    \n",
    "    #아핀변환으로 기울어진 그림을 정위치로 변환\n",
    "    img = cv.warpAffine(img,M,(SZ, SZ),flags=affine_flags)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![모멘트구조](./PostingPic/9_모멘트구조.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deskdew](./PostingPic/9_deskdew.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림 중 왼쪽은 오리지널 이미지, 오른쪽은 deskew 함수를 적용한 이미지이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG 함수 적용하기\n",
    "\n",
    "Next we have to find the HOG Descriptor of each cell. For that, we find Sobel derivatives of each cell in X and Y direction. Then find their magnitude and direction of gradient at each pixel. This gradient is quantized to 16 integer values. Divide this image to four sub-squares. For each sub-square, calculate the histogram of direction (16 bins) weighted with their magnitude. So each sub-square gives you a vector containing 16 values. Four such vectors (of four sub-squares) together gives us a feature vector containing 64 values. This is the feature vector we use to train our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x, y 디렉션에 대한 소벨 연산자 추출(가장자라)     \n",
    "- 각 픽셀의 gradient 방향과 정도를 추출\n",
    "- So each sub-square gives you a vector containing 16 values, = 16*4 = 64\n",
    "- 이렇게 뽑아낸 64개의 피쳐가 우리의 train data가 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog(img):\n",
    "    \n",
    "    gx = cv.Sobel(img, cv.CV_32F, 1, 0)\n",
    "    gy = cv.Sobel(img, cv.CV_32F, 0, 1)\n",
    "    \n",
    "    mag, ang = cv.cartToPolar(gx, gy)\n",
    "    \n",
    "    bins = np.int32(bin_n*ang/(2*np.pi))    # quantizing binvalues in (0...16)\n",
    "    \n",
    "    bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:]\n",
    "    mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]\n",
    "    \n",
    "    hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)]\n",
    "    hist = np.hstack(hists)     # hist is a 64 bit vector\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습에 적용하기\n",
    "\n",
    "- 위의 특징들을 가지고, 데이터셋에 학습을 적용해보자.\n",
    "\n",
    "[소스코드 풀버전](https://github.com/opencv/opencv/blob/master/samples/python/tutorial_code/ml/py_svm_opencv/hogsvm.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.2) /tmp/pip-req-build-yw7uvgqm/opencv/modules/core/src/utils/samples.cpp:62: error: (-2:Unspecified error) OpenCV samples: Can't find required data file: digits.png in function 'findFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-01782e30e3b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'digits.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.2) /tmp/pip-req-build-yw7uvgqm/opencv/modules/core/src/utils/samples.cpp:62: error: (-2:Unspecified error) OpenCV samples: Can't find required data file: digits.png in function 'findFile'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "SZ=20\n",
    "bin_n = 16 # Number of bins\n",
    "affine_flags = cv.WARP_INVERSE_MAP|cv.INTER_LINEAR\n",
    "\n",
    "def deskew(img):\n",
    "    m = cv.moments(img)\n",
    "    if abs(m['mu02']) < 1e-2:\n",
    "        return img.copy()\n",
    "    skew = m['mu11']/m['mu02']\n",
    "    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n",
    "    img = cv.warpAffine(img,M,(SZ, SZ),flags=affine_flags)\n",
    "    return img\n",
    "\n",
    "def hog(img):\n",
    "    gx = cv.Sobel(img, cv.CV_32F, 1, 0)\n",
    "    gy = cv.Sobel(img, cv.CV_32F, 0, 1)\n",
    "    mag, ang = cv.cartToPolar(gx, gy)\n",
    "    bins = np.int32(bin_n*ang/(2*np.pi))    # quantizing binvalues in (0...16)\n",
    "    bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:]\n",
    "    mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]\n",
    "    hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)]\n",
    "    hist = np.hstack(hists)     # hist is a 64 bit vector\n",
    "    return hist\n",
    "\n",
    "img = cv.imread(cv.samples.findFile('digits.png'),0)\n",
    "\n",
    "if img is None:\n",
    "    raise Exception(\"we need the digits.png image from samples/data here !\")\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(img,50)]\n",
    "# First half is trainData, remaining is testData\n",
    "train_cells = [ i[:50] for i in cells ]\n",
    "test_cells = [ i[50:] for i in cells]\n",
    "deskewed = [list(map(deskew,row)) for row in train_cells]\n",
    "hogdata = [list(map(hog,row)) for row in deskewed]\n",
    "trainData = np.float32(hogdata).reshape(-1,64)\n",
    "responses = np.repeat(np.arange(10),250)[:,np.newaxis]\n",
    "svm = cv.ml.SVM_create()\n",
    "svm.setKernel(cv.ml.SVM_LINEAR)\n",
    "svm.setType(cv.ml.SVM_C_SVC)\n",
    "svm.setC(2.67)\n",
    "svm.setGamma(5.383)\n",
    "svm.train(trainData, cv.ml.ROW_SAMPLE, responses)\n",
    "svm.save('svm_data.dat')\n",
    "deskewed = [list(map(deskew,row)) for row in test_cells]\n",
    "hogdata = [list(map(hog,row)) for row in deskewed]\n",
    "testData = np.float32(hogdata).reshape(-1,bin_n*4)\n",
    "result = svm.predict(testData)[1]\n",
    "mask = result==responses\n",
    "correct = np.count_nonzero(mask)\n",
    "print(correct*100.0/result.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 거의 94% 의 정확도가 나올 것이다. 다양한 파라미터를 적용하여, 더 높은 정확도가 나올 수 있도록 다양하게 트레이닝해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q5. OpenCV samples contain digits.py which applies a slight improvement of the above method to get improved result. It also contains the reference. Check it and understand it.\n",
    "\n",
    "opencv 샘플은 digits.py 파일을 포함하고 있습니다.. 이는 위의 결과보다 약간 더 향상된 결과를 내는 소스코드입니다. 이 샘플을 확인하고, 이해해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 오늘 실습은 여기까지입니다. \n",
    "\n",
    "###### 아무래도 책 내용이 정리된 것이 아니고, 단순히 openCV에서 제공하는 튜토리얼을 정리한 것이니 만큼 부족한 부분이 많을 것입니다. ㅠㅠㅠ 실습을 진행하시거나, 찬찬히 읽어보시면서 openCV에서의 머신러닝 활용에 대해 공부해보시고 토요일 오전에 뵙겠습니다!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
